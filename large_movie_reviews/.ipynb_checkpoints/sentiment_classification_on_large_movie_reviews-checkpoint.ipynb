{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification on large movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用 __[imdb Moview Reviews](http://ai.stanford.edu/~amaas/data/sentiment/)__ 的数据集，通过 Logistic Regression 算法进行情感分类(**Postive / Negative**)的划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备 Training set & Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据路径(full path)，读取文件中的文本内容\n",
    "def read_text(path):\n",
    "    return open(path, 'r').read()\n",
    "\n",
    "\n",
    "# 遍历指定目录，返回该目录下所有文件的 Path, Text 文本内容键值对的列表\n",
    "def read_files(directory):\n",
    "    for path in [join(directory, f) for f in listdir(directory)]:\n",
    "        yield path, read_text(path)\n",
    "\n",
    "\n",
    "# 根据指定目录下所有文件和分类定义，构建 DataFrame 数据集\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  class  \\\n",
      "aclImdb/train/pos/7923_8.txt   positive   \n",
      "aclImdb/train/pos/4567_10.txt  positive   \n",
      "aclImdb/train/pos/6347_9.txt   positive   \n",
      "aclImdb/train/neg/1037_1.txt   negative   \n",
      "aclImdb/train/neg/7389_2.txt   negative   \n",
      "aclImdb/train/neg/11143_1.txt  negative   \n",
      "aclImdb/train/pos/4680_10.txt  positive   \n",
      "aclImdb/train/pos/5630_8.txt   positive   \n",
      "aclImdb/train/neg/2548_1.txt   negative   \n",
      "aclImdb/train/neg/9350_1.txt   negative   \n",
      "\n",
      "                                                                            text  \n",
      "aclImdb/train/pos/7923_8.txt   While visiting Romania with his CIA dad, Tony(...  \n",
      "aclImdb/train/pos/4567_10.txt  My left foot is an epic outstanding film expla...  \n",
      "aclImdb/train/pos/6347_9.txt   How The Grinch Stole Christmas instantly stole...  \n",
      "aclImdb/train/neg/1037_1.txt   That this poor excuse for an amateur hour show...  \n",
      "aclImdb/train/neg/7389_2.txt   I buy or at least watch every Seagall movie. H...  \n",
      "aclImdb/train/neg/11143_1.txt  Hi, I'm a friend of werewolf movies, and when ...  \n",
      "aclImdb/train/pos/4680_10.txt  Terrfic film with a slightyly slow start - giv...  \n",
      "aclImdb/train/pos/5630_8.txt   Marjorie (a splendid and riveting performance ...  \n",
      "aclImdb/train/neg/2548_1.txt   The fact that this movie made it all the way t...  \n",
      "aclImdb/train/neg/9350_1.txt   This movie was terrible. The first half hour i...  \n",
      "                                  class  \\\n",
      "aclImdb/test/neg/3034_1.txt    negative   \n",
      "aclImdb/test/pos/10858_10.txt  positive   \n",
      "aclImdb/test/pos/142_8.txt     positive   \n",
      "aclImdb/test/neg/8984_1.txt    negative   \n",
      "aclImdb/test/neg/5802_4.txt    negative   \n",
      "aclImdb/test/pos/1107_9.txt    positive   \n",
      "aclImdb/test/pos/7075_10.txt   positive   \n",
      "aclImdb/test/neg/581_4.txt     negative   \n",
      "aclImdb/test/pos/7199_10.txt   positive   \n",
      "aclImdb/test/neg/9189_2.txt    negative   \n",
      "\n",
      "                                                                            text  \n",
      "aclImdb/test/neg/3034_1.txt    I've sat through less painful operations than ...  \n",
      "aclImdb/test/pos/10858_10.txt  I think Andrew Davies did an admirable job of ...  \n",
      "aclImdb/test/pos/142_8.txt     We all know what Chan-wook Park can do. If you...  \n",
      "aclImdb/test/neg/8984_1.txt    I don't leave IMDb comments about films but th...  \n",
      "aclImdb/test/neg/5802_4.txt    First of all, I'd like to say that I really en...  \n",
      "aclImdb/test/pos/1107_9.txt    UK-born Australian helmer Alex Frayne calls fo...  \n",
      "aclImdb/test/pos/7075_10.txt   A great story, although one we are certainly f...  \n",
      "aclImdb/test/neg/581_4.txt     \"May contain spoilers\" Sadly Lou Costellos' la...  \n",
      "aclImdb/test/pos/7199_10.txt   This film is the smartest comedy I have ever s...  \n",
      "aclImdb/test/neg/9189_2.txt    First off, I'm not a firefighter, but I'm in s...  \n"
     ]
    }
   ],
   "source": [
    "# 定义情感分类的 Labels\n",
    "POS = 'positive'\n",
    "NEG = 'negative'\n",
    "\n",
    "SOURCES = [\n",
    "    ('aclImdb/train/pos/', POS),\n",
    "    ('aclImdb/train/neg/', NEG)\n",
    "]\n",
    "# 准备训练数据\n",
    "train = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    train = train.append(build_data_frame(path, classification))\n",
    "\n",
    "# 将训练数据随机打乱\n",
    "train = train.reindex(np.random.permutation(train.index))\n",
    "\n",
    "TEST_SOURCES = [\n",
    "    ('aclImdb/test/pos/', POS),\n",
    "    ('aclImdb/test/neg/', NEG)\n",
    "]\n",
    "# 准备测试数据\n",
    "test = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in TEST_SOURCES:\n",
    "    test = test.append(build_data_frame(path, classification))\n",
    "    \n",
    "print(train.sample(10))\n",
    "print(test.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos:  12500\n",
      "Neg:  12500\n"
     ]
    }
   ],
   "source": [
    "print('Pos: ', len(train[train['class'] == POS]))\n",
    "print('Neg: ', len(train[train['class'] == NEG]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出 **Positive & Negative** 正反两类的训练数据刚好相等，因此可以不用针对分类的数据个数做 normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义分类算法的执行封装，并打印不同分类算法的耗时等信息\n",
    "# 根据之前的文本处理经验，默认使用 TfidfVectorizer 进行分词标记处理。\n",
    "def do_classify(tag, classifier, vectorizer=TfidfVectorizer()):\n",
    "    text_clf = Pipeline([('tfidf', vectorizer),\n",
    "                         (tag, classifier)])\n",
    "    print(tag + \" start...\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    text_clf = text_clf.fit(train['text'], train['class'])\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"   training time: \" + str(end_time - start_time))\n",
    "    predicted = text_clf.predict(test['text'])\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"   classification time: \" + str(end_time - start_time))\n",
    "    print(\"   accuracy: \", np.mean(predicted == test['class']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Logistic Regression 处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标准算法处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression start...\n",
      "   training time: 0:00:07.582187\n",
      "   classification time: 0:00:13.410926\n",
      "   accuracy:  0.88312\n"
     ]
    }
   ],
   "source": [
    "do_classify(\"LogisticRegression\", LogisticRegression(solver='liblinear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将之前的分词由单词扩展为至多由2个单词组成的词组的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression start...\n",
      "   training time: 0:00:34.893012\n",
      "   classification time: 0:00:50.044682\n",
      "   accuracy:  0.88628\n"
     ]
    }
   ],
   "source": [
    "do_classify(\"LogisticRegression\", LogisticRegression(solver='liblinear'), TfidfVectorizer(ngram_range=(1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出由于对分词的扩展造成了耗时的显著增加，然后准确率的提升并不明显"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加入对英文 Stemming 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression start...\n",
      "   training time: 0:01:44.900171\n",
      "   classification time: 0:03:27.072241\n",
      "   accuracy:  0.8812\n"
     ]
    }
   ],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "stem_vectorizer = TfidfVectorizer(analyzer=stemmed_words, ngram_range=(1, 2))\n",
    "\n",
    "do_classify(\"LogisticRegression\", LogisticRegression(solver='liblinear'), stem_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出 Stemming 仍然未对结果有一些正面影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试通过调参优化结果\n",
    "这里我尝试通过调整正则化项即罚函数（**Inverse of regularization strength**）进行优化，该项对模型向量进行“惩罚”，从而避免单纯最小二乘问题的过拟合问题。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'lr__C': (0.01, 0.1, 1.0, 10.0, 100.0)}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\n",
    "grid_obj = grid_obj.fit(train['text'], train['class'])\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('lr', LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False))], 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'lr': LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 'tfidf__analyzer': 'word', 'tfidf__binary': False, 'tfidf__decode_error': 'strict', 'tfidf__dtype': <class 'numpy.float64'>, 'tfidf__encoding': 'utf-8', 'tfidf__input': 'content', 'tfidf__lowercase': True, 'tfidf__max_df': 1.0, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1), 'tfidf__norm': 'l2', 'tfidf__preprocessor': None, 'tfidf__smooth_idf': True, 'tfidf__stop_words': None, 'tfidf__strip_accents': None, 'tfidf__sublinear_tf': False, 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__tokenizer': None, 'tfidf__use_idf': True, 'tfidf__vocabulary': None, 'lr__C': 10.0, 'lr__class_weight': None, 'lr__dual': False, 'lr__fit_intercept': True, 'lr__intercept_scaling': 1, 'lr__max_iter': 100, 'lr__multi_class': 'warn', 'lr__n_jobs': None, 'lr__penalty': 'l2', 'lr__random_state': None, 'lr__solver': 'liblinear', 'lr__tol': 0.0001, 'lr__verbose': 0, 'lr__warm_start': False}\n",
      "   accuracy:  0.88124\n"
     ]
    }
   ],
   "source": [
    "# 打印最佳的参数\n",
    "print(grid_obj.best_estimator_.get_params())\n",
    "# 用最佳方案预测结果\n",
    "predicted = clf.predict(test['text'])\n",
    "print(\"   accuracy: \", np.mean(predicted == test['class']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出结果仍然没有得到什么优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression 进行情感分类的算法准确率最后稳定在 88% 左右，通过一些数据处理和参数调优仍未发现有较大的优化空间。\n",
    "后续希望随着课程的深入，通过 Deep Learning 的算法优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
